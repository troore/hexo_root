---
title: FCCM'17 FP-DNN Review
date: 2017-07-15 16:33:45
mathjax: true
tags:
---

**Title**: FP-DNN: An Automated Framework for Mapping Deep Neural Networks onto FPGAs with RTL-HLS Hybrid Templates

**Conference**: FCCM 2017

**doi**: [http://ieeexplore.ieee.org/document/7966671/](http://ieeexplore.ieee.org/document/7966671/)

Summary
=======

FPGA-based accelerator is a promising solution for computation and communication intensive DNNs, considering its performance, flexibility and energy efficiency. Unfortunately, conventional accelerator design flows make it difficult for FPGA developers to keep up with the fast pace of innovations in DNNs. This paper proposes an automation framework to deploy DNNs described with software framework (TensorFlow) on FPGA. This framework is general enough to support various DNN models like CNN, LSTM, etc. It uses an optimized matrix multiplication unit to perform core computation for all models. It also applies communication optimization to improve off-chip memory access efficiency.

Strength
========

TensorFlow is a persuasive software library for mapping numerical computation depicted by flow graph onto hardware platforms, especially for machine learning applications. This work provides TensorFlow with FPGA support at the first time.

Given an application expressed by dataflow graph, and FPGA platform with hardware resource constraints, the authors formulate the minimization of number of off-chip buffers allocated for intermediate results generated by a graph node as a graph coloring problem.

This work implements a matrix multiplication (_MM_) unit as a kernel computation unit for a variety of layers in different models, such as convolutional layers, LSTM layers, etc. Its performance outperforms Intel's _MM_ hardware library and MKL software library.

For communication optimization, the authors propose a method called _Column-major_ to avoid duplicate DDR memory footprint as well as increase memory burst length to improve off-chip access bandwidth.

Weakness
========

The computation unit is implemented directly by RTL. It is not flexible and scalable for different requirement of DNN models, as well FPGA platform switching.

Although avoiding duplicate DDR memory footprint, the _Column-major_ method still needs redundant BRAM resources.

Detailed Comments
=================

**Hardware Implementation**

There is hardware resource usage of _Data Arranger_, but there is no running time for it.

_MM_ unit is implemented as fixed computation unit by RTL. Considering the fact that this work aims to map dataflow graphs like Figure 2(b) onto FPGA, if there are multiple implementations for different nodes, we have to manually implement them one by one. Thus it is not flexible and scalable for multi-FPGA extension.

Another problem is, the authors state that the fixed _MM_ could accelerate FC layers by batching. However, is batching is suggested for LSTM, given latency requirement?

As for the communiation optimization, the _Row-major_ method is useless to be illutrated in this paper. I don't think it is necessary to put it on purpose to illustrate the benefit of _Column-major_ way.

**Automation**

Why is the _Symbolic Compiler_ is written in C++ and OpenCL? We do not see any details on how to translate TensorFlow programming model to instantiate the hardware templates either.

**Experiment**

Only CNN performance is provided and compared with previous works. How to demonstrate its generalarity for other models, in terms of high performance?

What is authors' contribution to _MM_ performance optimization?

The most important motivation of the RTL implementation for _MM_ unit is that current designs cannot explore as much fine-grained optimization as in RTL designs. However, there is no illutration on what "fine-grained" optimization cannot be explored by current HLS tools. And there is no performance comparison with state-of-the-art HLS implementation of _MM_. As far as I know, the systolic array implementation of _MM_ in [Jie, et al., FPGA'15] could achieve more than 300 GFlops, which is comparative with the results in Table I in this paper.
